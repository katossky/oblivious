---
title: "Source (paper)"
author: "Arthur Katossky & LÃ©o Houairi"

format:
  pdf
# classoption: twocolumn
# reference-location: margin
# citation-location: margin
---


# Source (paper)
### Generating an oblivious random variable

> Given a data-point $(X, S)$ composed of non-sensitive and sensitive features $X$ and $S$ respectively, we can generate an oblivious random variable $Z$ as
$$\boldsymbol{Z}:=\phi(X)-E_n^S \phi(X)+E_n(\phi(X))$$
> Most kernel methods work with the kernel matrix and do not need access to the features themselves. The same holds in our setting. More specifically, we never need to represent $Z$ explicitly in the Hilbert space but only require inner-product calculations. In order to calculate the empirical estimates of the conditional expectation $E_n^S \phi(X)$ and of $E_n(\phi(X))$ in (9) we consider a simple approach whereby we split the training set into two subsets of size $n$, and use half the observations to obtain the empirical estimates of the expectations. The remaining $n$ observations are used to obtain an oblivious predictor; we have two cases as follows.

> Case 1 (M-Oblivious). The standard kernel matrix $K$ is calculated with the remaining $n$ observations and a kernel-method is applied to $K$ to obtain a predictor $g$. When applying the predictor to a new unseen data-point $(X, S)$ we first transform $X$ into $Z$ via (9) and calculate the prediction as $\langle g, \boldsymbol{Z}\rangle$. As discussed in the Introduction, we conjecture that this approach is suitable in the case where the labels $Y$ are conditionally independent of the sensitive features $S$ given the non-sensitive features $X$, i.e. when $S, X, Y$ form a Markov chain $S \rightarrow X \rightarrow Y$. As such we call this approach $M$-Oblivious.

> Case 2 (Oblivious). Instead of calculating the kernel matrix $K$ an oblivious kernel matrix, i.e.
$$\mathcal{O}=\left(\begin{array}{ccc}
\left\|\boldsymbol{Z}_1\right\|^2 & \cdots & \left\langle\boldsymbol{Z}_1, \boldsymbol{Z}_n\right\rangle \\
\vdots & \ddots & \vdots \\
\left\langle\boldsymbol{Z}_n, \boldsymbol{Z}_1\right\rangle & \cdots & \left\|\boldsymbol{Z}_n\right\|^2
\end{array}\right)$$
> is calculated by applying Equation (9) to the remaining training samples $\left(X_i, S_i\right)$ before taking inner products. The oblivious matrix is then passed to the kernel-method to gain a predictor $g$. The matrix is positive semi-definite since $\mathbf{a}^{\top} \mathcal{O} \mathbf{a}=\left\|\sum_{i=1}^n a_i \boldsymbol{Z}_i\right\|^2 \geq 0$, for any $\mathbf{a} \in \mathbb{R}^n$. The complexity to compute the matrix is $O\left(n^2\right)$ (see Appendix E for details on the algorithm). Prediction for a new unseen data-point $(X, S)$ is now done in the same way as in Case 1 .

### Application

> We carried out an experiment to mimic a scenario where a class of students should normally receive grades between 0 and 5 , and anyone with a grade above a fixed threshold $\theta=2$ should pass. Half of the class, representing a "minority group", are disadvantaged in that their grades are almost systematically reduced, while the other half receive a boost on average. More specifically, let the sensitive feature $S$ be a $\{0,1\}$-valued Bernoulli random variable with parameter $0.5$, and let $X_0$ be distributed according to a truncated normal distribution with support \[1,4\]. Let the non-sensitive feature $X$, representing a student's grade, be given by $$
> X:=\left(X_0-B\right) \chi\{S=0\}+\left(X_0+B\right) \chi\{S=1\}
> $$

> where $B$ is a Bernoulli random variable with parameter $0.9$ independent of $X_0$ and of $S$. The label $Y$ is defined as a noisy decision influenced by the student's "original grade" $X_0$ prior to the $S$-based modification. More formally, let $U$ be a random variable independent of $X_0$ and of $S$, and uniformly distributed on $[0,1]$. Let $Y_0:=\chi\left\{U \geq X_0\right\}$ and define $$
> Y:=Y_0 \chi\{X+S \geq \theta\} .
> $$ Classification Error. In a typical classification problem, the labels $Y$ depend on both $X$ and $S$ so when we remove the bias it is not clear what we should compare against when calculating the classification performance. Observe that our experimental construction here allows access to the true ground-truth labels $$
> Y^*:=\chi\left\{X_0 \geq \theta\right\} .
> $$ Therefore, we are able to calculate the true (unbiased) errors as well. However, this is not always the case in practice. In fact, we argue that the question of how to evaluate fair classification performance is an important open problem which has yet to be addressed.

> Measure of Dependence. Let $\mathcal{F}_n:=\sigma\left(X_1, \ldots, X_n, S_1, \ldots, S_n\right), n \in \mathbb{N}$ be the $\sigma$-algebra generated by the training samples. In this experiment, we measure the dependence between the predicted labels $\widehat{Y}$ produced by any algorithm and the sensitive features $S$ as $$
> \widetilde{\beta}(\widehat{Y}, S):=\frac{1}{2} \sum_{s \in\{0,1\}} \sum_{y \in\{0,1\}} E\left|P\left(\widehat{Y}=y, S=s \mid \mathcal{F}_n\right)-P\left(\widehat{Y}=y \mid \mathcal{F}_n\right) P(S=s)\right|
> $$ which is closely related to the $\beta$-dependence (see, e.g. (Bradley, 2007, vol. I, p. 67)) between their respective $\sigma$-algebras. We obtain an empirical estimate of $\widetilde{\beta}(\sigma(\widehat{Y}), \sigma(S))$ by simply replacing the probabilities in (14) with corresponding empirical frequencies.

> Experimental results. We generated $n=1000$ training and test samples as described above and the errors reported for each experiment are averaged over 10 repetitions. Figure 3 shows binary classification error vs. dependence between prediction and sensitive features for three different methods: classical Linear SVM, Linear FERM, and Oblivious SVM. In Figure 3a the error is calculated with respect to the observed labels which are intrinsically biased and in Figure 3a the error is calculated with respect to the true fair classification rule $Y^*$ given by (13). As can be seen in the plots, the true classification error of Oblivious SVM is smaller than that of the other two methods. Moreover, in both plots the $\beta$-dependence between the predicted labels produced by Oblivious SVM and the sensitive feature is close to 0 and 