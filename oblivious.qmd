---
title: "Oblivious data for fairness with kernels"
author: "Arthur Katossky & Léo Houairi"
bibliography: bibliography.bib
format:
  pdf:
    include-in-header:
      text: \usepackage{cancel}

# classoption: twocolumn
# reference-location: margin
# citation-location: margin
---

| Student   | Time     |
|----|-------|
| Léo   |     12h00 |
| Arthur   |     12h00 |

**TO DO**

Arthur

- Notations
- Transition p4/p5 (le gros du boulot)
- Encadré kernel metods


Léo

- Intro + annonce de plan
- Améliorer partie generating oblivious features
- Compléter exemple

A la fin

- Relecture notations (attention aux 2 produits scalaires)

\tableofcontents
\newpage 

::: {.callout-note}

léo: est-ce qu'on rajoute une petite note pour expliquer notre changement de notation par rapport à l'article (les x et z au lieu d'avoir des X et des Z partout). Et les tildes pour la partie sur la classification binaire, qui j'imagine servent à différencier de leurs contreparties empiriques

:::

In "Oblivious data for fairness with kernels" [@2021-oblivious], Steffen Grünewälder and Azadeh Khaleghi consider the prediction of an outcome $y$ from nonsensitive information $x$ where there exists some sensitive information $s$ that may be correlated with $x$. Dependence between $x$ and $s$ is a fairness issue if one wants to guarantee that predictions be independent from sensitive information $y\perp\!\!\!\perp s$. The authors restrict their discussion to kernel-based learning, that is the broad class of learning algorithms that, instead of relying directly on the sample $(x_1...x_n)$, may be expressed so as to rely only on a $n\times n$ positive semi-definite matrix $K_n$ summarizing the dependence between the observations where $K_{ij}=k(x_i,x_j)=\big\langle\varphi(x_i),\varphi(x_j)\big\rangle$ with $k:\mathbb{X}\times\mathbb{X}\to\mathbb{R}$ is called the _kernel_, $\varphi:\mathbb{X}\to\mathcal{H}$ is the associated _feature map_ and $\langle\cdot,\cdot\big\rangle$ is a inner product defined on $\mathcal{H}$.

In this context, they devise how to construct "oblivious" features $z=\phi(x)$ that both (a) guarantee the independence requirement $z \perp\!\!\!\perp s$ and (b) retain the information contained in $x$, in some maximal sense. Such features may be released and used for prediction, without sharing any confidential data. After showing that a strict respect of both criteria is infeasible, they provide relaxed constraints and approximate solutions. The $z$ features so constructed are to be used in the construction of the kernel matrix in place of $x$. **Parler du fait qu'on a pas besoin de calculer Z explicitement mais juste la matrice O**

The article being very technical and very thorough, we here propose a simplified version for the interested reader, with just enough to understand their application to binary classification (section 7.1). If you are familiar with (and not afraid by) [Reproducing Kernel Hilbert Spaces](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), [$P$-Donsker classes](https://fr.wikipedia.org/wiki/Classe_de_Donsker) or [Bochner-measurability](https://en.wikipedia.org/wiki/Bochner_measurable_function), please refer to the original article. The following report is organised in two sections, where we first introduce the theoretical background and then explore an application to binary classification with support vector machines.

## Background

Let's suppose we have at our disposal data $(x_n,s_n,y_n)$ where the $x$'s are nonsensitive features but the $S$'s are sensitive features. We want to provide a way to train a kernel-based model $f:\mathbb{X}\to\mathbb{Y}$ on $(x_n,y_n)$ where $s\perp\!\!\!\perp \hat y\equiv f(x)$ but we do not want our method to be dependent on which specific model $f$ we use. A wide-ranging solution is simply to replace $x$ by an other variable $z$, keep $x$ secret and perform the training on $(z_n,y_n)$ instead.

The goal of the article is thus to devise a procedure that allows to generate this new random variable $z$, that ideally should :

- be independent from $s$, a very hard constraint that the authors later relax
- be close to $x$, in some sense to be defined

The most straight-forward solution would be to define $z$ as the residual of the orthogonal projection of $x$ on $s$ : $$z \overset{\Delta}= x - \mathbb{E}(x \mid s) + \mathbb{E}x$$

However, in the specific context of kernel methods, the training of a model does not require access to the individual $x_i$s but rather only to the kernel matrix $K$, whose terms are $K_{ij}=k(x_i,x_j)=\langle\varphi(x_i),\varphi(x_j)\rangle$. So we might as well directly define $\mathbf{z}$ as the residual of the orthogonal projection of $\varphi(x)$ on $s$: $$\mathbf{z} \overset{\Delta}= \varphi(x) - \mathbb{E}\big[\varphi(x)\mid s\big] + \mathbb{E}\varphi(x)$${#eq-boldz}
... so that we get eventually $K_{ij}\simeq \langle\mathbf{z}_i,\mathbf{z}_j\rangle$ and $K\perp\!\!\!\perp (s_n)$.

The question are : why can't we guarantee strict independence? Do we loose anything when moving from $z$ to $\mathbf{z}$? What relaxation of the strict independence constraint can we guarantee? Note that each of theses answers are complicated by the fact that $\varphi$ (and thus $\mathbf{z}$) take values in a function space $\mathcal{H}\subseteq \mathbb{R}^{\mathbb{X}}$ that may not be finite.

::: {.callout-note}

## Mathematical setup and notations

We consider a probability space $(\Omega, \mathcal{A}, \mathbb{P})$ and a measurable space $(\mathbb{X}, \mathcal{X})$ (resp. $(\mathbb{S},\mathcal{S})$) in which the random variables $x$ and $z$ (resp. $s$) take their values.

For $a,b\in\mathbb{R}^d$, we note $a \cdot b = a^\top b$ the standard dot product.

:::


::: {.callout-note}

## Kernel methods

**Arthur complète**

Then, we consider $\mathcal{H}$ the reproducing kernel hilbert spaces composed of functions $h:\mathbb{X} \rightarrow \mathbb{R}$, its feature map being $\phi(x): X \rightarrow \mathcal{H}$. Finally, we consider the $\mathcal{L}^2$ space of functions attaining values in $\mathbb{H}$ (which is itself a space composed of functions). 

Kernels and RKHSs considered in the articles are:

- The **linear kernel** $\forall (a,b) \in\mathbb{R}^d: k(a,b)=a\cdot b$. The feature map is $\varphi(x)=x$. The RKHS $\mathcal{H}$ is the set of all functions $h_\mathbf{w}:x\mapsto x \cdot \mathbf{w}$ for $\mathbf{w}\in\mathbb{R}^d$, endowed with the scalar product $\langle h_\mathbf{u},h_\mathbf{v}\rangle=\mathbf{u}\cdot \mathbf{v}$. Noting that $f_\mathbf{w}=k(␣,\mathbf{w})$ and that $f_\mathbf{w}(x)=k(x,\mathbf{w})$, we can verify the reproducing property: $\forall h \in\mathcal{H}: \langle h ,k(␣,x)\rangle=h(x)$.

:::

### Independence is too strong a requirement

The ideal objective for oblivious features would be to construct a random variable $z$ with values in $\mathbb{X}$ that is both independent from $s$ and close to $x$. Assume distance is measured as the $L^2$ norm, we would define: $$z = \arg\min_{a\in L^2(\mathcal{X})}\|a-x\|_2\quad s.t. \quad a \perp\!\!\!\perp s$$

The authors rule out such a solution without much formalism :

> Completely removing the dependence of $x$ on $s$ without changing $x$ drastically is an intricate task that is rife with difficulties.

We provide the following toy example. Consider the case where $x=f(s)$ for some measurable function $f:\mathbb{S}\to\mathbb{X}$. As $a \perp\!\!\!\perp s \iff \forall g \text{ measurable}: a \perp\!\!\!\perp g(s)$, this in particular true for $f(s)\equiv x$. Thus we have $z \perp\!\!\!\perp x$, which together with $L^2$ minimisation leads to $z\overset{a.s.}=\mathbb{E}x$. This defeats the purpose of preserving the variations of $x$ in $z$.

Following the authors, we thus move from the strict independence assumption to the milder no-correlation assumption: $\mathbb{cov}(a_i,s_j)=0$ for each component of $a$ and $s$ (assuming both $\mathbb{X}$ and $\mathbb{S}$ are finite-dimensional). If $z$ and $s$ were Gaussian, this would be an equivalence, but it is not true in the general case.

### No-correlation between $z$ and $s$ is an excessive restriction

Decorrelation together with square-error minimisation is a well-known problem, whose solution is the residual of the projection of $x$ onto $s$ (where projection is defined from the vector product $\langle a,b\rangle=\mathbb{E}[ab]$) : 
$$z=x-\mathbb{E}\big(x\mid s\big) +\mathbb{E}x$$

However there are at least two reasons for not liking this solution:

1. Covariance is a bilinear operator. This is nice because we can take advantage of linearity in computation. But the price to pay is that all non-linear dependencies will be missed. 
2. More fundamentally, we need the $L^2$ norm to have a sense on $\mathcal{X}$, which may not be the case for many objects on which kernel-methods are defined.

For both reasons, we may want to move away from defining $z$ in the $\mathbb{X}$ space, and rather define $\mathbf{z}$ in the $\mathcal{H}$ space where the feature map $\varphi(x)$ lives. This helps on both sides as not only can covariance in an infinite feature space capture subtle forms of dependence that the linear case could not but we only have to define a distance on  $\varphi(x)$, which is easy since $\varphi(x)$ is a $\mathbb{R}^d$-valued function. \textbf{Vérifier que nous sommes cohérent sur les notations avec } $\mathcal{H}$ \textbf{ J'ai l'impression que parfois c'est l'ensemble des fonctions $\varphi$ parfois c'est l'espace où $\varphi$ prend valeur.}

### Relaxation

**A reformuler pour que ca colle avec le début jusqu'au début de "Generating oblivious features"**

we should just replace z by phi(z) but instead, we use boldz. More flexibility as boldz need not have any corresponding preimage by phi. phi(calz) is a manyfold that is contained into the Z space.

### Problem formulation and relaxations

At the begining, the problem considered is the construction of a random variable $Z: \Omega \rightarrow \mathbb{X}$ that is independant of $S$ and closer to $X$ than all other random variables respecting the independeance criterion. 

Then, the authors choose to do a first relaxation of the independance criterion. Instead of considering the independance as a criterion, they focus of the interactions of the random variables considered with functions (je paraphrase là). The independance criterion becomes that, $\forall h \in H, \quad \forall g \in L2$

$$E[h(Z)\times g(S)] = E[h(Z)] \times E[g(s)] $$
Because of the most important property of the RKHS, this condition can be rewritten, again $\forall h \in H, \quad \forall g \in L2$
$$E[\langle h, \phi(Z)\rangle \times g(S)] = E[ \langle h, \phi(Z)\rangle] \times E[g(s)] $$
This is interesting, because $\phi(Z)$ leaves in the space H but **does not cover all the space**. Indeed, if $\phi$ is continuous, $\phi(Z)$ is a low-dimensional manifold denoted $\mathcal{M}$ thereafter. 
Then, the authors chose to introduce a new relaxation. Instead of considering $\phi(Z)$, they replace it by a random variable $\mathbf{Z}: \Omega \rightarrow \mathbb{H}$. So, we move from the "two-steps" where we had $Z: \Omega \rightarrow \mathbb{X}$ and then $\phi(Z): \mathbb{X} \rightarrow \mathbb{H}$ to going directly from $\Omega$ to $\mathbb{H}$. 

After this relaxation, the authors defined the notion of $\mathbb{H}$**-Independence**. $\mathbf{Z}$ and $S$ are said $\mathbb{H}$-independent iff $h \in \mathbb{H}$ and all bounded measurable $g: \mathbb{S} \rightarrow \mathbb{R}$, we have: 
$$E[\langle h, \mathbf{Z}\rangle \times g(S)] = E[ \langle h, \mathbf{Z}\rangle] \times E[g(s)] $$
This criterion is very close to the precedent one, except that again, we go directly to $\mathbb{H}$. 

The problems becomes finding a $\mathbf{Z}$ that is $\mathbb{H}$-independent from S and as close as possible to $\phi(X)$ (in the $||\mathbf{Z} - \phi(X)||_2$, even if I am not sure to know what it means). 

The crucial point is to remark that if $\mathbf{Z}$ can be written as $\phi(W)$ for some $W$ in $\mathbb{X}$, then it is easy to show that $\mathbb{H}$-independence implies independence between a kernel estimator ($\langle \hat h, \mathbf{Z} \rangle$) and $S$. Of course, nothing guarantees that $\mathbf{Z}$ lies in the image of $\phi$ but, if it is close to this image (ie close to the manifold $\mathcal{M}$), it could be **projected** on the manifold. At the bottom of page 6, the authors are defining a notion of distance of $\mathbf{Z}$ to the manifold. Then, proposition 1 shows that $\exists W \in \mathbb{X}$ that attains this minimal value. 

Section 4 is mostly about estimating empirically the distance from $\mathbf{Z}$ to $\mathcal{M}$ and showing how to bound the approximation. 

In section 5, we are going back to the problem of chosing $\mathbf{Z}$. To do this, we recall the initial formula $Z = X - E[X|S] + E[X]$. But now we are working in the $\mathcal{M}$ space, so it simply becomes:
$$Z = \phi(X) - E[\phi(X)|S] + E[\phi(X)]$$
"Such feature is $\mathbb{H}$-independent from $S$. $\mathbf{Z}$ is the best approximation of $\phi(X)$ in the MSE under $\mathbb{H}$-independence. 


## Generating oblivious features 

### Two cases

Because the article focuses on kernel methods, the values of $\mathbf{z}$ never need to be computed, only their scalar products are required. The equation defining the oblivious features involves expectations and conditional expectations, that must be estimated from the data. The authors propose to split a dataset of length 2n in two sets of length n. The first half of the data is used to estimate these expectations using a plug-in approach while the second half can be used to compute a predictor. 

The authors distinguish two cases based on the dependency structure between $x$, $s$ and $y$.

**On se place dans le cas 2 oblivious qui est plus général, l'expliquer. Faire une petite note concernant le cas 1 M-oblivious.**

- **Case 1 (M-Oblivious)**. This is the case where $y|x$ is independent from $s$. The authors use the letter "M" because this resembles a Markovian setting in the sens that we have: $s \rightarrow x \rightarrow y$. In this case, a kernel is estimated from the second half of the data and then used to produce a predictor $g$, but the prediction on the new data point is done after transforming $x$ into $\mathbf{z}$. Still, $\mathbf{z}$ does not need to be computed explicitly for this new data point, because the prediction has the form $\langle g, \mathbf{z} \rangle$ \textbf{Arthur: toujours pas entièrement clair pour moi}
- **Case 2 Oblivious**. When $s$ can affect the label $y$ not only through $x$ but also through other means, the authors propose to compute an oblivious kernel matrix (denoted $\mathcal{O}$). Then this oblivious matrix can be used to create a predictor $g$, and the prediction for a new data point is done as in case 1.

### Computing the oblivious matrix

**ATTENTION erreur de notation: la premirère équation devarit être $\mathbf{z}_i = \varphi(x_i) - \mathbb{E}\big[\varphi(x)\mid s_i\big] + \mathbb{E}\varphi(x)$. Corriger tout ca dans cette partie**

The oblivious kernel matrix $\mathcal{O}$ is a n by n matrix containing the scalar products of the $\mathbf{z}$'s ($\mathcal{O}_{ij} = \langle \mathbf{z}_i, \mathbf{z}_j\rangle$). Computing this matrix boils down to computing $\langle \mathbf{z}_i, \mathbf{z}_j\rangle, \forall i, j$. Because of @eq-boldz, have: $\mathbf{z}_i = \varphi(x_i) - \mathbb{E}\big[\varphi(x_i)\mid s_i\big] + \mathbb{E}\varphi(x_i)$
So, the scalar product that we want to estimate is: 
$$\langle \varphi(x_i) - \mathbb{E}\big[\varphi(x_i)\mid s_i\big] + \mathbb{E}\varphi(x_i), \quad \varphi(x_j) - \mathbb{E}\big[\varphi(x_j)\mid s_j\big] + \mathbb{E}\varphi(x_j) \rangle$$
As mentioned above, the true expectations are replaced by empirical averages, yielding a computation of the scalar product as:
$$\mathcal{O}_{ij} = \langle \varphi(x_i) - \hat{\mathbb{E}}_n\big[\varphi(x_i)\mid s_i\big] + \hat{\mathbb{E}}_n\varphi(x_i), \quad \varphi(x_j) - \hat{\mathbb{E}}_n\big[\varphi(x_j)\mid s_j\big] + \hat{\mathbb{E}}_n\varphi(x_j) \rangle$${#eq-scalprod}

Using the properties of the scalar product, @eq-scalprod can be decomposed in nine terms: $\langle \varphi(x_i), \varphi(x_j)\rangle, \quad -\langle \varphi(x_i),\hat{\mathbb{E}}_n[\phi(x_j)|s_j]\rangle, \quad ...$. **C'est la 3ème la fois qu'on le dit, le changer d'une manière ou d'une autre **The expectations are replaced by empirical averages, and the conditional ones are remplaced by empirical averages involving indicator functions (recall that the empirical averages are computed on the first half of the data). For any event $S\in\mathcal{S}$ with strictly positive probability, a simple conditional expectation can be approached by the following empirical average (we use $\tilde{}$ to emphasise random variables) :
$$\hat{\mathbb{E}}_n(\varphi(\tilde x)|\tilde s \in S) = \frac{\sum_{i=1}^{n}\varphi(x_i) \times \mathbb{1}(s_i \in S)}{\sum_{i=1}^{n} \mathbb{1}(s_i \in S)}$$
The computation of each of the nine terms boils down to sums involving the kernel. In the article, appendix E1 details (some) the decompositions of the terms into kernel evaluation. As an example:
$$\langle \varphi(x_i), \hat{\mathbb{E}}_n(\varphi(x_j) | \tilde s \in S_j )\rangle = \frac{\sum_{l=1}^n k(x_i, x_l) \times \mathbb{1}(s_l \in S_j)}{\sum_{l=1}^n \mathbb{1}(s_l \in S_j)}$$

**Rajouter un lien logique avec la partie d'avant (et conclure sur le fait qu'on a pas à évaluer phi, parce que phi peut être infini)** It also provides a generic algorithm to compute the oblivious matrix, for a kernel $k(.,.)$ to be chosen. \textbf{[Arthur: note that the very computation is not trivial for continuous $s$ since we have to partition the range into chunks of non-zero probability; attention en pratique, y a pas mal de cas où si S est un peu bizarre ce sera compliqué. Faire 2 phrases là-dessus]}

## Application to binary classification 

**Enlever la référence au cas oblivious**

In this section, we present an example of the application of the method to the binary classification. We refer both to the text of the article and to the code in their implementation \footnote{https://github.com/azalk/Oblivious}. Some of the claims made in the paper, especially about the value of parameters, are contradicted by the implementation. In such case, we will refer to what we found in the implementation. 

The application is based on synthetic data. In an imaginary scenario, students receive a grade and are supposed to validate if this grade exceed an arbitrarily chosen threshold $\theta$. the students are discriminated according to some (binary) sensitive feature: $\tilde s\sim B(0.5)$ \footnote{In the implementation, half of the synthetic data has $s= 1$ and the other half $s=0$, deterministically}. We refer to the students with $s=0$ as the _minority group_ and to the other students as the _advantaged group_.

The original grade is denoted $\tilde x_0$ and it follows a Gaussian truncated between 1 and 4. Then, the grades are biased towards the advantaged group, which the authors model as:
$$\tilde x\overset{\Delta}=\tilde x_0+\tilde b\mathbb{1}(\tilde s=1)-\tilde b\mathbb{1}(\tilde s=0)$$
... where $\tilde b\sim B(0.9)$, a Bernoulli random variable, is the 1-unit bias for group $s=1$ or against group $s=0$. So, most students in the advantaged group will have $\tilde x > \tilde x_0$ and most students in the minority group will have $\tilde x < \tilde x_0$.

The decision whether a student should pass is denoted by the letter $y$. A fair rule should be to fix a threshold $\theta$ and have:
$$y^* \overset{\Delta}= \mathbb{1}(\tilde x_0 > \theta)$$
Instead, the unfair decision from the article is a mix of the student true ability (as given by $x_0$) and the biased one (as given by $x$):
$$\tilde y \overset{\Delta}= \mathbb{1}(\tilde u\leqslant \tilde x_0) \; \mathbb{1}(\tilde x+\tilde s\geqslant \theta)$$

... with $\tilde u \sim \mathcal{U}(1,4)$, or equivalently, $\mathbb{1}(\tilde u\leqslant x_0) \sim \mathcal{B}(x_0/4)$ for a known $x_0$\footnote{Actually, the article writes $\tilde y \overset{\Delta}= \mathbb{1}(\tilde u {\color{red}\geqslant} \tilde x_0) \; \mathbb{1}(\tilde x+\tilde s\geqslant \theta)$ but this writing makes little sense and seems to contradict their implementation. To us, it makes much more sense that the greater your initial grade is, the more likely you are to pass, and therefore to check the condition $\tilde u\geqslant \tilde x_0$ instead of $\tilde u\leqslant \tilde x_0$.}.

The example may seem contrived, but it is intentional from the authors' part. Observe that $\tilde {y}|\tilde{x}\,\cancel{\perp\!\!\!\perp}\,\tilde{s}$ and that this example thus falls in the \textbf{oblivious case}. Indeed, the minority group is disadvantaged twice in the process: (i) their grade $\tilde x$ can be diminished, (ii) as compared to the other group, their grade is compared to a higher threshold $\theta - \tilde s$, because $\tilde s = 1$ for the advantaged group. 


The authors also raise an important question: how should you evaluate a fair procedure ? Comparing the predictions to the real labels is not a good option because, by definition, the real labels are _biased_. In this synthetic case, the authors can compare their prediction with the "true", fair label, but it most of the time not possible in real life settings. They write:

> \[in our experiment\], we are able to calculate the true (unbiased) errors as well. However, this is not always the case in practice. In fact, we argue that the question of how to evaluate fair classification performance is an important open problem which has yet to be addressed.

**Finir cette section**
- Linear SVM
- Oblivious SVM (préciser le kernel)

Conclure avec leur résultat empirique, ie que le oblivious SVM garantit une quasi indépendance // linear SVM au prix d'une légère augmentation de l'erreur de classification. Préciser l'histoire de l'erreur de classification. 



--- 

**Fais comme tu veux arthur**
::: {.callout-note}

Waste of computation with kernel os size 2n if we use just the upper left and bottom right corners?
:::


The application uses synthetic data representing the grades of students discriminated according to some sensitive feature. $\tilde s\sim B(0.5)$ is the (binary) sensitive feature[^1]. The grades are biased towards the high-prestige status associated with $\tilde s=1$, which the authors model as: $$\tilde x\overset{def}=\tilde x_0+\tilde b\mathbb{1}(\tilde s=1)-\tilde b\mathbb{1}(\tilde s=0)$$ ... where $\tilde x_0\sim\mathcal{N}_{[1,4]}(2.5,0.5)$ is the baseline grade and $\tilde b\sim B(0.9)$ is the 1-unit bias for group $s=1$ or against group $s=0$ [^2].

\textbf{Sensitive feature in the code} In the code, half of the synthetic data has $s=1$ the other half $s=0$, deterministically:

```python
unique_sensitive_feature_values=[0,1]
sensitive_features =
[unique_sensitive_feature_values[0]] * n_samples +
[unique_sensitive_feature_values[1]] * n_samples
```
    
\textbf{X0 in the code} In the code:

```python
max_non_sensitive_feature_value = 4.0
min_non_sensitive_feature_value = 1.0
mu = 0.5 * (max_non_sensitive_feature_value+min_non_sensitive_feature_value)
Lower = generate_truncnorm_samples(
n_samples, min_non_sensitive_feature_value,
max_non_sensitive_feature_value, mu, sigma
)
Upper = generate_truncnorm_samples(
n_samples, min_non_sensitive_feature_value,
max_non_sensitive_feature_value, mu, sigma
)
non_sensitive_features = 
[Lower-stats.bernoulli(0.9).rvs(n_samples)*1]+
[Upper+stats.bernoulli(0.9).rvs(n_samples)*1]
```

Eventually, the authors construct a decision mimicking some decision made based on the students' official results. This decision is based on a mix of the true student abilities (as given by $x_0$) and the biased ones (as given by $x$). Namely, they set : $$\tilde y = \mathbb{1}(\tilde u\geqslant \tilde x_0) \; \mathbb{1}(\tilde x+\tilde s\geqslant \theta)$$ ... with $\tilde u\sim\mathcal{U}[0,4]$ and $\theta$ a real parameter, arbitrarily set by the authors to be $\theta=\mathbb{E}\tilde x_0=2.5$ [^4].

\textbf{In the code: contradiciton regarding U} The paper states that $\tilde u\sim\mathcal{U}[0,1]$ but this contradicts the earlier statement that $\tilde x_0$ be truncated on $[1,4]$. In the code, we see that $\mathbb{1}(\tilde u\geqslant x_0)\sim \mathcal{B}(x_0/4)$, which means that in reality $\tilde u\sim\mathcal{U}[0,4]$. Again, this ignores the fact that $\tilde x_0\overset{a.s.}>1$.
    
```python
threshold = mu # mu is the mean of the truncated normal distribution from above
Y_Bernoulli_params = non_sensitive_features0 / max_non_sensitive_feature_value
Y = [stats.bernoulli(Y_Bernoulli_params[i]).rvs(1) for i in range(len(Y_Bernoulli_params))]
Y = Y * ((non_sensitive_features + sensitive_features >= threshold)*1)
```
    
Note that the exemple falls in the **Oblivious** case, that is that $Y|X$ is not independent from $S$. Indeed, the minority group is disavantdged twice in the process:

- First, their grade $\tilde x$ is diminished.
- Secondly, their grade is compared to a higher threshold: $\theta - \tilde s$. Indeed, for the advantaged group, $\tilde s = 1$.      

Contrary to the mathematical background, the implementation is not detailed in the paper, so we reconstruct it from the code. In [the SVM example]( https://github.com/azalk/Oblivious/blob/main/SVM_Example.py ), the substantial part is:

```python
# X is training data (including sensitive data) of length 2n = 2×500 
# S is X reduced to the sensitive data
# y_train is training predictions of length n
K = obl.build_K_lin(X,X)      # K has size 2n × 2n
O = obl.build_O_discrete(K,S) # O has size  n ×  n
clf = svm.SVC(kernel='precomputed')
clf.fit(O, y_train)
```

The `K = obl.build_K_lin(X,X)` line is straightforward, as it just builds the kernel matrix consisting in all dot products between line $i$ and line $j$ of `X`. On the other hand, the essence of the oblivious method proposed by the authors lay hidden behind the `O = obl.build_O_discrete(K,S)` line. It boils down to the following loop over the first $n=500$ observations, where all the averages `mean_iI`, `mean_IJ`, `mean_I`, `mean_i` and `Ephi` are computed on the second (independent) half. (This explains why `K` has size $2n$ × $2n$ whereas `O` has size $n\times n$.)

```python
for i in range(n): # n=500 (half the dataset length)
  for j in range(i,n):
    
    u = int(self.S_train[i]) # value of s for observation i
    v = int(self.S_train[j]) # value of s for observation j
    
    O[i,j] = K[i,j] # kernel value
      # 1. subtract individual-category average K[i, S=s]
      #    for s matching observed values
      - mean_iI[i,v] - mean_iI[j,u] 
      # 2. add category-category average K[S=s1,S=s2]
      #    for s1, s2 matching observed values
      + mean_IJ[u,v]
      # 3. add individual average K[i,.]
      + mean_i[j] + mean_i[i]
      # 4. subtract category average K[S=s,.]
      #    with s matching observed values
      - mean_I[u] - mean_I[v]
      # 5. add general average K[.,.]
      + Ephi
    
    # save computation with symmetry
    O[j,i] = O[i,j]
```
---
