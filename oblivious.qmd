---
title: "Oblivious data for fairness with kernels"
author: "Arthur Katossky & Léo Houairi"
bibliography: bibliography.bib
format:
  pdf:
    toc: true
    toc-depth: 3
    include-in-header:
      text: \usepackage{cancel}

# classoption: twocolumn
# reference-location: margin
# citation-location: margin
---

## Introduction

In "Oblivious data for fairness with kernels" [@2021-oblivious], Steffen Grünewälder and Azadeh Khaleghi consider the prediction of an outcome $y$ from nonsensitive information $x$ where there exists some sensitive information $s$ that may be correlated with $x$. Dependence between $x$ and $s$ is a fairness issue if one wants to guarantee that predictions be independent from sensitive information: $y\perp\!\!\!\perp s$. The authors restrict their discussion to kernel-based learning, that is the broad class of learning algorithms that, instead of relying directly on the sample $(x_1...x_n)$, may be expressed so as to rely only on a $n\times n$ positive semi-definite matrix $K_n$ summarizing the dependence between the observations where $K_{ij}=k(x_i,x_j)=\big\langle\varphi(x_i),\varphi(x_j)\big\rangle$ with $k:\mathbb{X}\times\mathbb{X}\to\mathbb{R}$ is called the _kernel_, $\varphi:\mathbb{X}\to\mathcal{H}$ is the associated _feature map_ and $\langle\cdot,\cdot\big\rangle$ is an inner product defined on $\mathcal{H}$.

In this context, they devise how to construct "oblivious" features $z=f(x,s)$ (for some decorrelation function $f$) that both (a) guarantee the independence requirement $z \perp\!\!\!\perp s$ and (b) retain the information contained in $x$, in some maximal sense. These $z$ features are to be used in the construction of the kernel matrix in place of $x$ and yet, they  may be released and used for prediction, without sharing any confidential data! Because the authors are only interested in kernel methods, it is only necessary to compute $O_n=\big(\langle \varphi(z_i),\varphi(z_j)\rangle\big)_{i,j=1..n}$. It will never be necessary to explicitly compute $\mathbf{z}=\varphi(z)=\varphi(f(x,s))$. After showing that a strict respect of both criteria (a) and (b) is infeasible, they provide relaxed constraints and approximate solutions.

The article being very technical and very thorough, we here propose a simplified version for the interested reader; please refer to the original article for more information. The following report is organised in three sections. We first introduce the theoretical background, then explain how to generate oblivious feature and compute the oblivious kernel matrix and finally present as exemple the application of the method to binary classification. 

## Background

Let's suppose we have at our disposal data $(x_n,s_n,y_n)$ where the $x$'s are nonsensitive features but the $s$'s are sensitive features. We want to provide a way to train a kernel-based model $g:\mathbb{X}\to\mathbb{Y}$ on $(x_n,y_n)$ where $s\perp\!\!\!\perp \hat y\equiv g(x)$ but we do not want our method to be dependent on which specific model $f$ we use. A wide-ranging solution is simply to replace $x$ by an other variable $z$, keep $x$ secret and perform the training on $(z_n,y_n)$ instead.

The goal of the article is thus to devise a procedure that allows to generate this new random variable $z$, that ideally should :

- be independent from $s$, a very hard constraint that the authors later relax
- be close to $x$, in some sense to be defined

The most straight-forward solution would be to define $z$ as the residual of the orthogonal projection of $x$ on $s$ : $$z \overset{\Delta}= x - \mathbb{E}(x \mid s) + \mathbb{E}x$$

However, in the specific context of kernel methods, the training of a model does not require access to the individual $x_i$s but rather only to the kernel matrix $K$, whose terms are $K_{ij}=k(x_i,x_j)=\langle\varphi(x_i),\varphi(x_j)\rangle$. So we might as well directly define $\mathbf{z}$ as the residual of the orthogonal projection of $\varphi(x)$ on $s$: $$\mathbf{z} \overset{\Delta}= \varphi(x) - \mathbb{E}\big[\varphi(x)\mid s\big] + \mathbb{E}\varphi(x)$${#eq-boldz}
... so that we get eventually $K_{ij}\simeq \langle\mathbf{z}_i,\mathbf{z}_j\rangle$ and $K\perp\!\!\!\perp (s_n)$.

The question are : why can't we guarantee strict independence? Do we loose anything when moving from $z$ to $\mathbf{z}$? What relaxation of the strict independence constraint can we guarantee? Note that each of theses answers are complicated by the fact that $\varphi$ (and thus $\mathbf{z}$) take values in a function space $\mathcal{H}\subseteq \mathbb{R}^{\mathbb{X}}$ that may not be finite.

::: {.callout-note}

## Mathematical setup and notations

We consider a probability space $(\Omega, \mathcal{A}, \mathbb{P})$ and a measurable space $(\mathbb{X}, \mathcal{X})$ (resp. $(\mathbb{S},\mathcal{S})$) in which the random variables $x$ and $z$ (resp. $s$) take their values. $x$ is an observed variable. $s$ is some sensitive information. $z$ a public release of $x$ that ideally should not depend on $s$. When needed, we highlight by a $\tilde{}$ the random variables, in order to distinguish them from their realisations. Thus, formally, $x\in\mathbb{X}$ but $\tilde x\in\mathbb{X}^\Omega$.

We note $\mathcal{H}$ the reproducing kernel Hilbert space (RKHS) composed of functions $h:\mathbb{X} \rightarrow \mathbb{H}$ where $\mathbb{H}\overset{\Delta}=\mathbb{R}^d$, while $\phi(x): X \rightarrow \mathcal{H}$ is its feature map and $k(\cdot,\cdot)$ its associated kernel. We note $a \cdot b , \forall (a,b) \in \mathbb{H}^2$ the standard dot product and $\langle h_1,h_2 \rangle, \forall (h_1,h_2)\in\mathcal{H}^2$ the RKHS inner product. We use bold font in $\mathbf{x}$ and $\mathbf{z}$ for the representations of $x$ and $z$ in the feature space $\mathcal{H}$.

In a RKHS, we have both:

- $k(a,b)=\langle \varphi(a),\varphi(b) \rangle=\langle \varphi(b),\varphi(a) \rangle$
- $\forall h\in\mathcal{H},\forall x\in\mathbb{X}:  h(x)=\langle h,\varphi(x) \rangle$

For instance, the **linear kernel** is such that $\forall (a,b) \in\mathbb{R}^d: k(a,b)=a\cdot b$. The feature map is $\varphi(x)=x$. The RKHS $\mathcal{H}$ is the set of all functions $h_\mathbf{w}:x\mapsto x \cdot \mathbf{w}$ for $\mathbf{w}\in\mathbb{R}^d$, endowed with the scalar product $\langle h_\mathbf{u},h_\mathbf{v}\rangle=\mathbf{u}\cdot \mathbf{v}$. Noting that $f_\mathbf{w}=k(␣,\mathbf{w})$ and that $f_\mathbf{w}(x)=k(x,\mathbf{w})$, we can verify the reproducing property: $\forall h \in\mathcal{H}: \langle h ,k(␣,x)\rangle=h(x)$.

Contrary to the article, we do not consider specific notations for the randomness of reproducing kernel Hilbert space.

We write data as $(y_i,x_i)_{i=1..n}$ though we never consider any $y_i$ explicitly. We write $g$ any predictor using kernel methods, belonging to a class $\mathcal{G}$. The "best" $g^\star$ is estimated on a sample, but its dependence on $(x_i)_{i=1..n}$ can completely be expressed by the kernel matrix $K_n=(k(x_i,x_j))_{i,j=1..n}$. In the article, this kernel matrix will be replaced by its oblivious counterpart $O_n$ or — when we realise that this is infeasible — an approximation $\mathcal{O}_n$. We note $\hat y=g^\star(x)$.
:::

### Independence is too strong a requirement

The ideal objective for oblivious features would be to construct a random variable $z$ with values in $\mathbb{X}$ that is both independent from $s$ and close to $x$. Assume distance is measured as the $L^2$ norm, we would define: $$z = \arg\min_{a\in L^2(\mathcal{X})}\|a-x\|_2\quad s.t. \quad a \perp\!\!\!\perp s$$

The authors rule out such a solution without much formalism :

> Completely removing the dependence of $x$ on $s$ without changing $x$ drastically is an intricate task that is rife with difficulties.

We provide the following toy example. Consider the case where $x=f(s)$ for some measurable function $f:\mathbb{S}\to\mathbb{X}$. As $a \perp\!\!\!\perp s \iff \forall g \text{ measurable}: a \perp\!\!\!\perp g(s)$, this in particular true for $f(s)\equiv x$. Thus we have $z \perp\!\!\!\perp x$, which together with $L^2$ minimisation leads to $z\overset{a.s.}=\mathbb{E}x$. This defeats the purpose of preserving the variations of $x$ in $z$.

Following the authors, we thus move from the strict independence assumption to the milder no-correlation assumption: $\mathbb{cov}(a_i,s_j)=0$ for each component of $a$ and $s$ (assuming both $\mathbb{X}$ and $\mathbb{S}$ are finite-dimensional). If $z$ and $s$ were Gaussian, this would be an equivalence, but it is not true in the general case.

### No-correlation between $z$ and $s$ is an excessive restriction

Decorrelation together with square-error minimisation is a well-known problem, whose solution is the residual of the projection of $x$ onto $s$ : 
$$z=x-\mathbb{E}\big(x\mid s\big) +\mathbb{E}x$$

However there are at least two reasons for not liking this solution:

1. Covariance is a bilinear operator. This is nice because we can take advantage of linearity in computation. But the price to pay is that all non-linear dependencies will be missed. 
2. More fundamentally, we need the $L^2$ norm to have a sense on $\mathcal{X}$, which may not be the case for many objects on which kernel-methods are defined.

For both reasons, we may want to move away from defining $z$ in the $\mathbb{X}$ space, and rather define $\mathbf{z}$ in the $\mathcal{H}$ space where the feature map $\varphi(x)$ lives.

### Framing shift

We used to have $\mathbf{z}=\varphi(z)=\varphi(f(x,s))$ where $f:\mathbb{X}\times \mathbb{S}\to\mathbb{X}$ is some decorrelation function. Since this problem is unsolvable, we now want a new decorrelation function $f':\mathcal{H}\times \mathbb{S}\to \mathcal{H}$ and directly pick $\mathbf{z}=f'(\mathbf{x},s)=f'(\varphi(x),s)$.

This helps on both sides as not only can covariance in the (potentially infinite) feature space $\mathcal{H}\subseteq (\mathbb{R}^d)^{\mathbb{X}}$ capture subtle forms of dependence that the linear case could not, but we only have to define distances on $\mathcal{H}$, which is easy since $\varphi(x)\in\mathcal{H}$ is an $\mathbb{R}^d$-valued function.

However, this is an important departure from the initial problem. Specifically, we have no guarantee whatsoever that $\mathbf{z}=f'(\mathbf{x},s)$ has a pre-image through $\varphi$, meaning that we may end up with a decorrelated $\mathbf{z}\in\mathcal{H}$ that does not correspond to any actual $z\in\mathbb{X}$. Authors call $\mathcal{M}=\varphi(\mathbb{X})\subseteq\mathcal{H}$ the manifold that contains all image of elements $x\in\mathbb{X}$ through $\varphi$. They show that if $\mathbf{z}$ is close to the manifold $\mathcal{M}$, it can be meaningfully **projected** onto the manifold. After carefully defining a distance to $\mathcal{M}$, they prove that there exists a $z \in \mathbb{X}$ that attains this minimal value (proposition 1).

### Alternative independence definition

But now how to define the independence requirement for $\mathbf{z}\in\mathcal{H}$? Indeed, that's from an independence definition that we obtained $f$ in the first place ! The authors proceed step by step, starting with the premise that independence can be defined in terms of expectation: $$\begin{aligned}
\tilde z\perp\!\!\!\perp \tilde s \iff \\
\forall h_1 \in (\mathbb{R}^d)^{\mathbb{X}}, \; \forall h_2 \in (\mathbb{R}^d)^{\mathbb{S}} : \\
\mathbb{E}[h_1(\tilde z)\cdot h_2(\tilde s)]=\mathbb{E}[h_1(\tilde z)]\cdot \mathbb{E}[h_2(\tilde s)]
\end{aligned}$$

The thing is that for any $h\in\mathcal{H}\subseteq (\mathbb{R}^d)^{\mathbb{X}}$, the reproducing property $h(z)=\langle h,\varphi(z)\rangle$ holds. Thus, the former expression can be amended as^[We here show that the former expression (valid for $h_1 \in (\mathbb{R}^d)^{\mathbb{X}}$) implies the latter (valid only for $h_1 \in \mathcal{H}\subseteq (\mathbb{R}^d)^{\mathbb{X}}$). The equivalence seems plausible but we do not venture in a proof.]:$$\forall h_1 \in \mathcal{H}, \forall h_2 \in (\mathbb{R}^d)^{\mathbb{S}} :  \mathbb{E}[\langle h_1,\varphi(\tilde z)\rangle \cdot h_2(\tilde s)]=\mathbb{E}[\langle h_1,\varphi(\tilde z)\rangle] \cdot \mathbb{E}[h_2(\tilde s)]$$

After noting that $\varphi(z)$ does not cover the full space $\mathcal{H}$ but on the contrary is restricted to $\mathcal{M}=\varphi(\mathbb{X})$, the last step is to authorise for any value outside of $\mathcal{M}$. This independence criterion is called $\mathbb{H}$-independence by the authors : $$\forall h_1 \in \mathcal{H}, \forall h_2 \in (\mathbb{R}^d)^{\mathbb{S}} :  \mathbb{E}[\langle h_1,\tilde{\mathbf{z}}\rangle  \cdot h_2(\tilde s)]=\mathbb{E}[\langle h_1,\tilde{\mathbf{z}}\rangle] \cdot \mathbb{E}[h_2(\tilde s)]$$

At this step, we have completely moved away from defining the independence in the $\mathbb{X}$ space, to defining it in the feature space $\mathcal{H}$. In this context, the decorrelation problem becomes finding a $\mathbf{z}$ that is $\mathbb{H}$-independent from $s$ and as close as possible to $\phi(x)$, in the sense that $\|\mathbf{z} - \phi(x)\|_2$ is minimal. $\mathbb{H}$-independence is not as stringent as strict independence^[If $\exists z\in\mathbb{X}: \varphi(z)=\mathbf{z}$, $\mathbb{H}$-independence between $\mathbf{z}$ and $s$ implies strict independence between $h_1(\mathbf{z})$ and $s$, with the same notations as in the rest of the section.], and we are likely to find a solution close to $\mathcal{M}$, for which this solution is meaningful. Solving these constraints move us back where we started: $$\mathbf{z}=\varphi(x)-\mathbb{E}[\varphi(x)|s]+\mathbb{E}[\varphi(x)]$$

The difference is that now, we have theoretical grounding for our initial intuition.

## Practical computations

### Overview

The equation defining the oblivious features involves expectations and conditional expectations, that must be estimated from the data. But we do not need to compute these exact quantities. Because the article focuses on kernel methods, we are only interested in the $\mathbf{z}$'s scalar products, which together form the oblivious kernel matrix $\mathcal{O}=\big(\langle\mathbf{z}_i,\mathbf{z}_j\rangle\big)_{ij=1..n}$.

In practice, the authors propose a two-step method to build $\mathcal{O}$. They split a dataset of length $2n$ in two sets of length $n$. The first half of the data is used to estimate the expectations using a plug-in approach while the second half is used to compute the kernel matric $\mathcal{O}$. Then, $\mathcal{O}$ can be used to compute any kernel-based predictor $g$^[
This description corresponds to the most general case, called "Oblivious" by the authors. In this situation, $y$ is affected by $s$ through $x$ but also through other means (this will be clearer in the example of binary classification). Therefore, $\tilde {y}|\tilde{x}\,\cancel{\perp\!\!\!\perp}\,\tilde{s}$.
The authors mention an other case, deemed "M-Oblivious", where $s$ affects $y$ only through $x$. This set-up resembles markov chains, because $s \rightarrow x \rightarrow y$ and so $y|x \perp s$. In this case, the kernel matrix can be estimated on the second half of the original data (without transformation into $\mathbf{z}$).
].

<!-- When a prediction for a new point $x_{new}$ is required, $x_{new}$ is first transformed into $\mathbf{z}_{new}$ and the prediction has the form $\langle g, \mathbf{z_{new}} \rangle \simeq g(x_{new})$. Again, $\mathbf{z}_{new}$ does not need to be computed explicitly for this new data point, because the prediction takes the form of a scalar product. -->


### Computing the oblivious matrix

Because of @eq-boldz, we have: $\mathbf{z}_i = \varphi(x_i) - \mathbb{E}\big[\varphi(x)\mid s_i\big] + \mathbb{E}\varphi(x)$. Thus, computing $\mathcal{O}_{ij}=\langle\mathbf{z}_i,\mathbf{z}_j\rangle$ boils down to estimating : 
$$\langle \varphi(x_i) - \mathbb{E}\big[\varphi(x)\mid s_i\big] + \mathbb{E}\varphi(x), \quad \varphi(x_j) - \mathbb{E}\big[\varphi(x)\mid s_j\big] + \mathbb{E}\varphi(x) \rangle$$
As mentioned above, in the estimation process the true expectations are replaced by empirical averages and the conditional expectations are replaced by conditional empirical averages, yielding an empirical counterpart of the scalar product as:
$$\left(\hat{\mathcal{O}}_n\right)_{ij} = \langle \varphi(x_i) - \hat{\mathbb{E}}_n\big[\varphi(\tilde x)\mid \tilde s=s_i\big] + \hat{\mathbb{E}}_n\varphi(\tilde x), \quad \varphi(x_j) - \hat{\mathbb{E}}_n\big[\varphi(\tilde x)\mid \tilde s=s_j\big] + \hat{\mathbb{E}}_n\varphi(\tilde x) \rangle$${#eq-scalprod}

Using the properties of the scalar product, @eq-scalprod can be decomposed in nine terms: $\langle \varphi(x_i), \varphi(x_j)\rangle, \quad -\langle \varphi(x_i),\hat{\mathbb{E}}_n[\varphi(\tilde x)|\tilde s = s_j]\rangle, \quad ...$, all of which need to be computed. 

Note that for any event $S\in\mathcal{S}$ with strictly positive probability, a simple conditional expectation can be approached by the following empirical average (we use $\tilde{}$ to emphasise random variables) :
$$\hat{\mathbb{E}}_n(\varphi(\tilde x)|\tilde s \in S) = \frac{\sum_{i=1}^{n}\varphi(x_i) \times \mathbb{1}(s_i \in S)}{\sum_{i=1}^{n} \mathbb{1}(s_i \in S)}$$
The computation of each of the nine terms boils down to sums involving the kernel. In the article, appendix E1 details (some) the decompositions of the terms into kernel evaluation. As an example:
$$\langle \varphi(x_i), \hat{\mathbb{E}}_n(\varphi(x) | \tilde s \in S_j )\rangle = \frac{\sum_{l=1}^n k(x_i, x_l) \times \mathbb{1}(s_l \in S_j)}{\sum_{l=1}^n \mathbb{1}(s_l \in S_j)}$$

At this point, we see that each scalar product in the oblivious matrix can be estimated by empirical averages of different sorts that involve calls to the kernel $k(.,.)$ but no explicit computation of $\mathbf{z}_i$. (This is a nice property ! Recall that $\varphi$ takes values in a space that is potentially non-finite, typically a function space.) Appendix E1 presents the full algorithm to produce the oblivious kernel matrix $\hat{\mathcal{O}}_n$ for a given kernel $k$, aggregating all of these computations in one pass.

:::{.callout-warning}
Note that depending on the nature of $s$, the computation can be more or less trivial. If the sensitive attribute $s$ is discrete and balanced, there should not be any issue. If it is continuous, or discrete with numerous / unbalanced categories, then it is necessary to partition the range into classes of non-zero probability and guarantee that each class is large large enough to contain enough observations in the sample. The authors do not comment on this issue. 
:::

## Application to binary classification with one binary sensitive feature

In this section, we present an example of the application of method to the binary classification. We refer both to the text of the article and to the code in teh authors' implementation.  \footnote{https://github.com/azalk/Oblivious}. Some of the claims made in the paper, especially about the value of parameters, are contradicted by the implementation. In such case, we will refer to what we found in the implementation. 

The application is based on synthetic data. In an imaginary scenario, students receive a grade and are supposed to validate if this grade exceed an arbitrarily chosen threshold $\theta$. The students are discriminated according to some (binary) sensitive feature: $\tilde s\sim B(0.5)$ \footnote{In the implementation, half of the synthetic data has $s= 1$ and the other half $s=0$, deterministically.}. We refer to the students with $s=0$ as the _minority group_ and to the other students as the _advantaged group_.

The original grade is denoted $\tilde x_0$ and it follows a Gaussian truncated between 1 and 4. Then, the grades are biased towards the advantaged group, which the authors model as:
$$\tilde x\overset{\Delta}=\tilde x_0+\tilde b\mathbb{1}(\tilde s=1)-\tilde b\mathbb{1}(\tilde s=0)$$
... where $\tilde b\sim B(0.9)$, a Bernoulli random variable, is the 1-unit bias for group $s=1$ or against group $s=0$. So, most students in the advantaged group will be advantaged: $\tilde x > \tilde x_0$ and most students in the minority group will be disadvantaged: $\tilde x < \tilde x_0$.

The decision whether a student should pass is denoted by the letter $y$. Given a threshold $\theta$, a fair rule should be:
$$y^* \overset{\Delta}= \mathbb{1}(\tilde x_0 \geqslant \theta)$$
Instead, the unfair decision from the article is a mix of the student true ability (as given by $\tilde x_0$) and the biased one (as given by $\tilde x$):
$$\tilde y \overset{\Delta}= \mathbb{1}(\tilde u\leqslant \tilde x_0) \; \mathbb{1}(\tilde x+\tilde s\geqslant \theta)$$

... with $\tilde u \sim \mathcal{U}(1,4)$, or equivalently, $\mathbb{1}(\tilde u\leqslant x_0) \sim \mathcal{B}(x_0/4)$ for a known $x_0$\footnote{Actually, the article writes $\tilde y \overset{\Delta}= \mathbb{1}(\tilde u {\color{red}\geqslant} \tilde x_0) \; \mathbb{1}(\tilde x+\tilde s\geqslant \theta)$ but this writing makes little sense and seems to contradict their implementation. To us, it makes much more sense that the greater your initial grade is, the more likely you are to pass, and therefore to check the condition $\tilde u\leqslant \tilde x_0$ instead of $\tilde u\geqslant \tilde x_0$.}.

The example may seem contrived, but it illustrates a case where $y$ is affected by $s$ both through $x$ and directly. Observe that $\tilde {y}|\tilde{x}\,\cancel{\perp\!\!\!\perp}\,\tilde{s}$. Indeed, the minority group is disadvantaged twice in the process: (i) their grade $\tilde x$ can be diminished (influence through $\tilde x$), (ii) as compared to the other group, their grade is compared to a higher threshold $\theta - \tilde s$, because $\tilde s = 1$ for the advantaged group (direct influence).

In this setup, the authors fit to the data: (i) a linear SVM, so the kernel is simply the linear kernel $k(x_i, x_j) = x_i \cdot x_j$, (ii) an oblivious SVM, so the linear kernel is still used, but is applied to $\mathbf{z}$. Then, they compare those two methods in terms of binary classification error.

At this point, the authors raise an important question: how should such _fair_ procedure be evaluated ? Comparing the predicted labels to the observed labels is not a good option because, by definition, the observed labels are _biased_. In this toy example, the authors have at their disposal the _fair_ labels (as defined by $y^*$), but this is not the case in most practical settings. The authors write:

> \[in our experiment\], we are able to calculate the true (unbiased) errors as well. However, this is not always the case in practice. In fact, we argue that the question of how to evaluate fair classification performance is an important open problem which has yet to be addressed.

In their application, the authors show that although the misclassification error measured with the real (observed, biased) labels is higher using oblivious SVM (as compared to linear SVM), it is lower when the error is measured using the fair labels. 

Finally, the authors measure the dependency between the predictions of the algorithm $\hat y$ and the sensitive feature $\tilde s$ using a measure $\beta$ that writes:
$$\beta(\hat y, \tilde s) \overset{\Delta}= \frac{1}{2} \sum_{s \in \{0,1\}} \sum_{y \in \{0,1\}} \mathbb{E} \lvert \mathbb{P}(\hat y = y, \tilde s = s \vert \mathcal{F}_n) - \mathbb{P}(\hat y = y\vert \mathcal{F}_n) \mathbb{P}(\tilde s = s)\rvert $$
... where $\mathcal{F}_n$ is the $\sigma$-algebra generated by the training set. (Here, both $\hat y$ and $\tilde s$ are considered as random variables, not a specific sample observation.) Intuitively, $\beta$ should le be small if $\hat y$ and $\tilde s$ are close to independent. The authors compute the empirical counterpart of $\beta$ and show that $\hat \beta \simeq 0$ for the oblivious SVM, whereas $\hat \beta \geqslant 0.8$ for the linear SVM.

In this synthetic example, the method proposed by the authors allows to reach something close to independence between the sensitive attributes and the prediction of the algorithms. The misclassification error of the algorithm is higher than a typical linear SVM if the labels used for the evaluation are the observed ones, but lower if the _fair_ labels are used.

---

## References

::: {#refs}
:::